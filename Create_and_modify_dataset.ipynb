{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccad0dd4",
   "metadata": {},
   "source": [
    "# Import et variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28afed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io.wavfile as wav\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "PATH_PROJECT_ROBIN = \"/home/robin/Bureau/ETUDES/M2/S2/TAP/\"\n",
    "PATH_PROJECT_QUENTIN = \"...\"\n",
    "PATH_PROJECT_PERSONNE = PATH_PROJECT_ROBIN\n",
    "PATH_RAW_DATASET = \"Projet-Traitement-Automatique-de-la-Parole/Dataset/\"\n",
    "GLOBAL_PATH_DATASET = os.path.join(PATH_PROJECT_PERSONNE, PATH_RAW_DATASET)\n",
    "GLOBAL_PATH_RAW_DATASET = os.path.join(GLOBAL_PATH_DATASET, \"speech_commands_v0.02/\")\n",
    "GLOBAL_PATH_BASIC_DATASET = os.path.join(GLOBAL_PATH_DATASET, \"Dataset_basique/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b4ee6",
   "metadata": {},
   "source": [
    "# Get filenames for partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ac62653",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(GLOBAL_PATH_RAW_DATASET, \"testing_list.txt\"), 'r') as f:\n",
    "    contenu = f.read()\n",
    "    testing_list = contenu.split(\"\\n\")\n",
    "with open(os.path.join(GLOBAL_PATH_RAW_DATASET, \"validation_list.txt\"), 'r') as f:\n",
    "    contenu = f.read()\n",
    "    validation_list = contenu.split(\"\\n\")\n",
    "sound_dirs = [elem for elem in os.listdir(GLOBAL_PATH_RAW_DATASET) if os.path.isdir(os.path.join(GLOBAL_PATH_RAW_DATASET,elem))]\n",
    "label2id = {\"up\" : 0, \"down\" : 1, \"left\" : 2, \"right\" : 3, \"unknown\" : 4, \"silence\" : 5}\n",
    "id2label = {v : k for k,v in label2id.items()}\n",
    "labels = label2id.keys()\n",
    "dataset_train = []\n",
    "dataset_validation = []\n",
    "dataset_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7296d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sound_dir in sound_dirs:\n",
    "    path_to_dir = os.path.join(GLOBAL_PATH_RAW_DATASET, sound_dir)\n",
    "    if \"background\" in sound_dir :\n",
    "        pass\n",
    "    else :\n",
    "        if sound_dir in labels :\n",
    "            label = sound_dir\n",
    "        else :\n",
    "            label = \"unknown\"\n",
    "        for sound_file in os.listdir(path_to_dir):\n",
    "            if sound_file.endswith('0.wav') or sound_file.endswith('1.wav') :\n",
    "                path_to_file = os.path.join(path_to_dir, sound_file)\n",
    "                total_filename = os.path.join(sound_dir,sound_file)\n",
    "                if total_filename in validation_list :\n",
    "                    dataset_validation.append((label, total_filename))\n",
    "                elif total_filename in testing_list :\n",
    "                    dataset_test.append((label, total_filename))\n",
    "                else :\n",
    "                    dataset_train.append((label, total_filename))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cc4ed79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  :  {'unknown': 51040, 'down': 2043, 'up': 1937, 'left': 1979, 'right': 1994}\n",
      "validation  :  {'unknown': 6116, 'down': 262, 'up': 241, 'left': 237, 'right': 249}\n",
      "test  :  {'unknown': 6335, 'down': 262, 'up': 257, 'left': 264, 'right': 260}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_size_dataset(lists, list_names):\n",
    "    dict_save = {}\n",
    "    for k, list in enumerate(lists) :\n",
    "        dict_save[list_names[k]] = {}\n",
    "        dict_aff = {}\n",
    "        for label, filename in list:\n",
    "            if label in dict_aff:\n",
    "                dict_aff[label] += 1\n",
    "                dict_save[list_names[k]][label].append(filename)\n",
    "            else :\n",
    "                dict_aff[label] = 1\n",
    "                dict_save[list_names[k]][label] = [filename]\n",
    "        print(list_names[k],\" : \",dict_aff)\n",
    "    return dict_save[\"train\"], dict_save[\"validation\"], dict_save[\"test\"]\n",
    "train_names, validation_names, test_names = check_size_dataset([dataset_train,dataset_validation,dataset_test]\n",
    "                                                               , [\"train\", \"validation\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "049a5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_names['unknown'])\n",
    "random.shuffle(validation_names['unknown'])\n",
    "random.shuffle(test_names['unknown'])\n",
    "train_names['unknown'] = train_names['unknown'][:2000]\n",
    "validation_names['unknown'] = validation_names['unknown'][:250]\n",
    "test_names['unknown'] = test_names['unknown'][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c57e3490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succès : 9953 fichiers ont été sauvegardés dans '/home/robin/Bureau/ETUDES/M2/S2/TAP/Projet-Traitement-Automatique-de-la-Parole/Dataset/speech_commands_v0.02/train_list_class.txt'.\n",
      "Succès : 1239 fichiers ont été sauvegardés dans '/home/robin/Bureau/ETUDES/M2/S2/TAP/Projet-Traitement-Automatique-de-la-Parole/Dataset/speech_commands_v0.02/validation_list_class.txt'.\n",
      "Succès : 1293 fichiers ont été sauvegardés dans '/home/robin/Bureau/ETUDES/M2/S2/TAP/Projet-Traitement-Automatique-de-la-Parole/Dataset/speech_commands_v0.02/test_list_class.txt'.\n"
     ]
    }
   ],
   "source": [
    "def save_dict_to_txt(dict_data, output_filenames):\n",
    "    for i in range(len(dict_data)):\n",
    "        count = 0\n",
    "        with open(output_filenames[i], \"w\", encoding=\"utf-8\") as f:\n",
    "            for label, file_list in dict_data[i].items():\n",
    "                for filename in file_list:\n",
    "                    f.write(f\"{filename}\\n\")\n",
    "                    count += 1\n",
    "        print(f\"Succès : {count} fichiers ont été sauvegardés dans '{output_filenames[i]}'.\")\n",
    "dict_save_list = [train_names, validation_names, test_names]\n",
    "filename_save_list = [os.path.join(GLOBAL_PATH_RAW_DATASET,\"train_list_class.txt\"),\n",
    "                      os.path.join(GLOBAL_PATH_RAW_DATASET,\"validation_list_class.txt\"),\n",
    "                      os.path.join(GLOBAL_PATH_RAW_DATASET,\"test_list_class.txt\")]\n",
    "save_dict_to_txt(dict_save_list, filename_save_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a1f7b",
   "metadata": {},
   "source": [
    "# Create base dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "749e24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(GLOBAL_PATH_RAW_DATASET, \"test_list_class.txt\"), 'r') as f:\n",
    "    contenu = f.read()\n",
    "    testing_list = contenu.split(\"\\n\")\n",
    "with open(os.path.join(GLOBAL_PATH_RAW_DATASET, \"validation_list_class.txt\"), 'r') as f:\n",
    "    contenu = f.read()\n",
    "    validation_list = contenu.split(\"\\n\")\n",
    "with open(os.path.join(GLOBAL_PATH_RAW_DATASET, \"train_list_class.txt\"), 'r') as f:\n",
    "    contenu = f.read()\n",
    "    train_list = contenu.split(\"\\n\")\n",
    "with open(os.path.join(GLOBAL_PATH_RAW_DATASET, \"test_silence_list.txt\"), 'r') as f:\n",
    "    contenu = f.read()\n",
    "    testing_list.extend(contenu.split(\"\\n\"))\n",
    "with open(os.path.join(GLOBAL_PATH_RAW_DATASET, \"val_silence_list.txt\"), 'r') as f:\n",
    "    contenu = f.read()\n",
    "    validation_list.extend(contenu.split(\"\\n\"))\n",
    "with open(os.path.join(GLOBAL_PATH_RAW_DATASET, \"train_silence_list.txt\"), 'r') as f:\n",
    "    contenu = f.read()\n",
    "    train_list.extend(contenu.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a20d106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_dirs = [elem for elem in os.listdir(GLOBAL_PATH_RAW_DATASET) if os.path.isdir(os.path.join(GLOBAL_PATH_RAW_DATASET,elem))]\n",
    "label2id = {\"up\" : 0, \"down\" : 1, \"left\" : 2, \"right\" : 3, \"unknown\" : 4, \"silence\" : 5}\n",
    "id2label = {v : k for k,v in label2id.items()}\n",
    "labels = label2id.keys()\n",
    "dataset_train = []\n",
    "dataset_validation = []\n",
    "dataset_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef4167ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sound_dir in sound_dirs:\n",
    "    path_to_dir = os.path.join(GLOBAL_PATH_RAW_DATASET, sound_dir)\n",
    "    if not (\"background\" in sound_dir) :\n",
    "        if sound_dir in labels :\n",
    "            label = label2id[sound_dir]\n",
    "        else :\n",
    "            label = label2id[\"unknown\"]\n",
    "        for sound_file in os.listdir(path_to_dir):\n",
    "            if sound_file.endswith('0.wav') or sound_file.endswith('1.wav') or \"silence\" in sound_dir:\n",
    "                path_to_file = os.path.join(path_to_dir, sound_file)\n",
    "                total_filename = os.path.join(sound_dir,sound_file)\n",
    "                fs, sig = wav.read(path_to_file)\n",
    "                duree = len(sig) / fs\n",
    "                if duree != 1 :\n",
    "                    sig = np.pad(sig, (0, 16000 - len(sig)), mode='constant')\n",
    "                if total_filename in validation_list :\n",
    "                    dataset_validation.append((sig, fs, len(sig) / fs, label, total_filename))\n",
    "                elif total_filename in testing_list :\n",
    "                    dataset_test.append((sig, fs, len(sig) / fs, label, total_filename))\n",
    "                elif total_filename in train_list :\n",
    "                    dataset_train.append((sig, fs, len(sig) / fs, label, total_filename))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b26877ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  :  {4: 2000, 1: 2043, 0: 1937, 5: 2000, 2: 1979, 3: 1994}\n",
      "validation  :  {4: 250, 1: 262, 0: 241, 5: 250, 2: 237, 3: 249}\n",
      "test  :  {4: 250, 1: 262, 0: 257, 5: 250, 2: 264, 3: 260}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_size_dataset(lists, list_names):\n",
    "    dict_save = {}\n",
    "    for k, list in enumerate(lists) :\n",
    "        dict_save[list_names[k]] = {}\n",
    "        dict_aff = {}\n",
    "        for (sig, fs, duree, label, filename) in list:\n",
    "            if label in dict_aff:\n",
    "                dict_aff[label] += 1\n",
    "                dict_save[list_names[k]][label].append(filename)\n",
    "            else :\n",
    "                dict_aff[label] = 1\n",
    "                dict_save[list_names[k]][label] = [filename]\n",
    "        print(list_names[k],\" : \",dict_aff)\n",
    "    return dict_save[\"train\"], dict_save[\"validation\"], dict_save[\"test\"]\n",
    "train_names, validation_names, test_names = check_size_dataset([dataset_train,dataset_validation,dataset_test], [\"train\", \"validation\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f08a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_list):\n",
    "    X, Y, metadata = [], [], []\n",
    "    for data in dataset_list :\n",
    "        X.append(data[0])\n",
    "        Y.append(data[3])\n",
    "        metadata.append({\"fs\": data[1], \"duree\": data[2], \"filename\" : data[4]})\n",
    "    return X, Y, metadata\n",
    "\n",
    "X_train, Y_train, meta_train = get_data(dataset_train)\n",
    "X_val, Y_val, meta_val = get_data(dataset_validation)\n",
    "X_test, Y_test, meta_test = get_data(dataset_test)\n",
    "\n",
    "dataset_dict = {\n",
    "    \"train\": {\n",
    "        \"signals\": X_train, \n",
    "        \"labels\": Y_train, \n",
    "        \"metadata\": meta_train\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"signals\": X_val,   \n",
    "        \"labels\": Y_val,   \n",
    "        \"metadata\": meta_val\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"signals\": X_test,  \n",
    "        \"labels\": Y_test,  \n",
    "        \"metadata\": meta_test\n",
    "    },\n",
    "    \"mapping\": label2id\n",
    "}\n",
    "\n",
    "filename = os.path.join(GLOBAL_PATH_DATASET, \"dataset_basique.pkl\")\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(dataset_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tap_tp_partie1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
